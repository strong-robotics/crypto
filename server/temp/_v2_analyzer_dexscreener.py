#!/usr/bin/env python3

import asyncio
import aiohttp
# SQLite (BACKUP - commented out)
# import aiosqlite
import json
import random
from typing import Dict, Any, Optional, List
from datetime import datetime
# PostgreSQL (ACTIVE)
from _v2_db_pool import get_db_pool
from config import config

class DexScreenerAnalyzer:
    def __init__(self, db_path: str = "db/tokens.db", debug: bool = False):
        # SQLite (BACKUP - commented out)
        # import os
        # os.makedirs(os.path.dirname(db_path), exist_ok=True)
        # self.db_path = db_path
        # self.conn: Optional[aiosqlite.Connection] = None
        # self.db_lock = asyncio.Lock()
        
        self.session: Optional[aiohttp.ClientSession] = None
        self.debug = debug
        
        self.is_scanning = False
        self.scan_interval = config.DEXSCREENER_ANALYZER_INTERVAL
        self.scan_task: Optional[asyncio.Task] = None
        self.batch_size = config.DEXSCREENER_ANALYZER_BATCH_SIZE
        
    async def ensure_connection(self):
        """PostgreSQL - pool created globally"""
        pass
        
    async def ensure_session(self):
        if self.session is None:
            self.session = aiohttp.ClientSession()
    
    async def close(self):
        if self.session:
            await self.session.close()
    
    async def _fetch_with_retries(self, url: str, **kwargs) -> Dict[str, Any]:
        last_exc = None
        for attempt in range(1, 4):
            try:
                async with self.session.get(url, **kwargs) as resp:
                    status = resp.status
                    text = await resp.text()
                    try:
                        parsed = json.loads(text)
                    except Exception:
                        parsed = None
                    if 200 <= status < 300:
                        return {"ok": True, "status": status, "json": parsed}
                    else:
                        return {"ok": False, "status": status, "json": parsed, "error": f"HTTP {status}"}
            except Exception as e:
                last_exc = e
                backoff = 0.4 * (2 ** (attempt - 1)) * (1 + random.random() * 0.3)
                await asyncio.sleep(backoff)
        return {"ok": False, "error": str(last_exc)}
    
    async def get_token_data(self, token_address: str) -> Any:
        try:
            await self.ensure_session()
            url = f"{config.DEXSCREENER_API}?q={token_address}"
            res = await self._fetch_with_retries(url)
            if res["ok"]:
                return res["json"]
            return {"error": res.get("error")}
        except Exception as e:
            return {"error": str(e)}
    
    def extract_pair_address(self, dexscreener_data: Any) -> Optional[str]:
        try:
            if isinstance(dexscreener_data, dict):
                pairs = dexscreener_data.get("pairs") or []
                if isinstance(pairs, list) and pairs:
                    return pairs[0].get("pairAddress")
            return None
        except Exception:
            return None

    async def save_dexscreener_data(self, token_id: int, token_address: str, dexscreener_data: Dict[str, Any]) -> bool:
        try:
            if not isinstance(dexscreener_data, dict):
                if self.debug:
                    print(f"‚ö†Ô∏è DexScreener data is not a dict for {token_address}")
                return False
            
            pairs = dexscreener_data.get("pairs", [])
            if not pairs or not isinstance(pairs, list):
                if self.debug:
                    print(f"‚ö†Ô∏è No pairs found for {token_address}")
                return False
            
            pair = pairs[0]
            pool = await get_db_pool()
            
            async with pool.acquire() as conn:
                pair_address = pair.get("pairAddress")
                if pair_address:
                    await conn.execute("""
                        UPDATE token_ids SET token_pair = $1 WHERE id = $2
                    """, pair_address, token_id)
                
                # PostgreSQL UPSERT with float conversion for Decimal fields
                await conn.execute("""
                    INSERT INTO dexscreener_pairs (
                        token_id, chain_id, dex_id, url, pair_address,
                        price_native, price_usd, fdv, market_cap, pair_created_at, timestamp
                    ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10, CURRENT_TIMESTAMP)
                    ON CONFLICT (token_id) DO UPDATE SET
                        chain_id = EXCLUDED.chain_id,
                        dex_id = EXCLUDED.dex_id,
                        url = EXCLUDED.url,
                        pair_address = EXCLUDED.pair_address,
                        price_native = EXCLUDED.price_native,
                        price_usd = EXCLUDED.price_usd,
                        fdv = EXCLUDED.fdv,
                        market_cap = EXCLUDED.market_cap,
                        pair_created_at = EXCLUDED.pair_created_at,
                        timestamp = CURRENT_TIMESTAMP
                """,
                    token_id,
                    pair.get("chainId"),
                    pair.get("dexId"),
                    pair.get("url"),
                    pair_address,
                    str(pair.get("priceNative", "")) if pair.get("priceNative") else None,
                    str(pair.get("priceUsd", "")) if pair.get("priceUsd") else None,
                    float(pair.get("fdv", 0)) if pair.get("fdv") else None,
                    float(pair.get("marketCap", 0)) if pair.get("marketCap") else None,
                    # pair_created_at –≤ PostgreSQL ‚Äî —Ü–µ TEXT, —Ç–æ–º—É –∫–æ–Ω–≤–µ—Ä—Ç—É—î–º–æ datetime ‚Üí str
                    datetime.fromtimestamp(pair.get("pairCreatedAt", 0) / 1000).isoformat() if pair.get("pairCreatedAt") else None
                )
                
                base_token = pair.get("baseToken", {})
                if base_token:
                    await conn.execute("""
                        INSERT INTO dexscreener_base_token (token_id, address, name, symbol)
                        VALUES ($1, $2, $3, $4)
                        ON CONFLICT (token_id) DO UPDATE SET
                            address = EXCLUDED.address,
                            name = EXCLUDED.name,
                            symbol = EXCLUDED.symbol
                    """, token_id, base_token.get("address"), base_token.get("name"), base_token.get("symbol"))
                
                quote_token = pair.get("quoteToken", {})
                if quote_token:
                    await conn.execute("""
                        INSERT INTO dexscreener_quote_token (token_id, address, name, symbol)
                        VALUES ($1, $2, $3, $4)
                        ON CONFLICT (token_id) DO UPDATE SET
                            address = EXCLUDED.address,
                            name = EXCLUDED.name,
                            symbol = EXCLUDED.symbol
                    """, token_id, quote_token.get("address"), quote_token.get("name"), quote_token.get("symbol"))
                
                txns = pair.get("txns", {})
                if txns:
                    m5 = txns.get("m5", {})
                    h1 = txns.get("h1", {})
                    h6 = txns.get("h6", {})
                    h24 = txns.get("h24", {})
                    await conn.execute("""
                        INSERT INTO dexscreener_txns (
                            token_id, m5_buys, m5_sells, h1_buys, h1_sells,
                            h6_buys, h6_sells, h24_buys, h24_sells
                        ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9)
                        ON CONFLICT (token_id) DO UPDATE SET
                            m5_buys = EXCLUDED.m5_buys,
                            m5_sells = EXCLUDED.m5_sells,
                            h1_buys = EXCLUDED.h1_buys,
                            h1_sells = EXCLUDED.h1_sells,
                            h6_buys = EXCLUDED.h6_buys,
                            h6_sells = EXCLUDED.h6_sells,
                            h24_buys = EXCLUDED.h24_buys,
                            h24_sells = EXCLUDED.h24_sells
                    """,
                        token_id,
                        m5.get("buys", 0), m5.get("sells", 0),
                        h1.get("buys", 0), h1.get("sells", 0),
                        h6.get("buys", 0), h6.get("sells", 0),
                        h24.get("buys", 0), h24.get("sells", 0)
                    )
                
                volume = pair.get("volume", {})
                if volume:
                    await conn.execute("""
                        INSERT INTO dexscreener_volume (token_id, h24, h6, h1, m5)
                        VALUES ($1, $2, $3, $4, $5)
                        ON CONFLICT (token_id) DO UPDATE SET
                            h24 = EXCLUDED.h24,
                            h6 = EXCLUDED.h6,
                            h1 = EXCLUDED.h1,
                            m5 = EXCLUDED.m5
                    """, token_id,
                        float(volume.get("h24", 0)) if volume.get("h24") else None,
                        float(volume.get("h6", 0)) if volume.get("h6") else None,
                        float(volume.get("h1", 0)) if volume.get("h1") else None,
                        float(volume.get("m5", 0)) if volume.get("m5") else None
                    )
                
                price_change = pair.get("priceChange", {})
                if price_change:
                    await conn.execute("""
                        INSERT INTO dexscreener_price_change (token_id, m5, h1, h6, h24)
                        VALUES ($1, $2, $3, $4, $5)
                        ON CONFLICT (token_id) DO UPDATE SET
                            m5 = EXCLUDED.m5,
                            h1 = EXCLUDED.h1,
                            h6 = EXCLUDED.h6,
                            h24 = EXCLUDED.h24
                    """, token_id,
                        float(price_change.get("m5", 0)) if price_change.get("m5") else None,
                        float(price_change.get("h1", 0)) if price_change.get("h1") else None,
                        float(price_change.get("h6", 0)) if price_change.get("h6") else None,
                        float(price_change.get("h24", 0)) if price_change.get("h24") else None
                    )
                
                liquidity = pair.get("liquidity", {})
                if liquidity:
                    await conn.execute("""
                        INSERT INTO dexscreener_liquidity (token_id, usd, base, quote)
                        VALUES ($1, $2, $3, $4)
                        ON CONFLICT (token_id) DO UPDATE SET
                            usd = EXCLUDED.usd,
                            base = EXCLUDED.base,
                            quote = EXCLUDED.quote
                    """, token_id,
                        float(liquidity.get("usd", 0)) if liquidity.get("usd") else None,
                        float(liquidity.get("base", 0)) if liquidity.get("base") else None,
                        float(liquidity.get("quote", 0)) if liquidity.get("quote") else None
                    )
                
                # Increment check_dexscreener only on successful data update
                await conn.execute("""
                    UPDATE token_ids 
                    SET check_dexscreener = LEAST(check_dexscreener + 1, 3)
                    WHERE id = $1
                """, token_id)

                if self.debug:
                    print(f"‚úÖ Saved DexScreener data for token_id={token_id}, address={token_address}")
                
                return True
                
        except Exception as e:
            if self.debug:
                print(f"‚ùå Error saving DexScreener data for {token_address}: {e}")
            return False
    
    async def get_tokens_to_scan(self, limit: int = 5) -> List[Dict[str, Any]]:
        try:
            pool = await get_db_pool()
            
            async with pool.acquire() as conn:
                rows = await conn.fetch("""
                    SELECT id, token_address, check_dexscreener
                    FROM token_ids
                    WHERE check_dexscreener < 3
                    ORDER BY created_at ASC
                    LIMIT $1
                """, limit)
                
                tokens = []
                for row in rows:
                    tokens.append({
                        "token_id": row['id'],
                        "token_address": row['token_address'],
                        "check_dexscreener": row['check_dexscreener']
                    })
                
                return tokens
                
        except Exception as e:
            if self.debug:
                print(f"‚ùå Error getting tokens to scan: {e}")
            return []
    
    async def scan_token(self, token: Dict[str, Any]) -> bool:
        try:
            token_id = token["token_id"]
            token_address = token["token_address"]
            
            if self.debug:
                print(f"üîç Scanning token_id={token_id}, address={token_address}, check={token['check_dexscreener']}")
            
            dexscreener_data = await self.get_token_data(token_address)
            
            # Check for API error
            if "error" in dexscreener_data:
                if self.debug:
                    print(f"‚ö†Ô∏è DexScreener API error for {token_address}: {dexscreener_data['error']}")
                return False
            
            # Try to save data (increments check_dexscreener on success)
            success = await self.save_dexscreener_data(token_id, token_address, dexscreener_data)
            if not success and self.debug:
                print(f"‚ö†Ô∏è No pairs found for {token_address}")
            
            return success
            
        except Exception as e:
            if self.debug:
                print(f"‚ùå Error scanning token {token.get('token_address')}: {e}")
            return False
    
    async def _auto_scan_loop(self):
        while self.is_scanning:
            try:
                tokens = await self.get_tokens_to_scan(limit=self.batch_size)
                
                if not tokens:
                    if self.debug:
                        print(f"‚ÑπÔ∏è No tokens to scan (all have check_dexscreener >= 3)")
                    await asyncio.sleep(self.scan_interval)
                    continue
                
                if self.debug:
                    print(f"üìä Scanning {len(tokens)} tokens...")
                
                tasks = [self.scan_token(token) for token in tokens]
                results = await asyncio.gather(*tasks, return_exceptions=True)
                
                success_count = sum(1 for r in results if r is True)
                
                if self.debug:
                    print(f"‚úÖ Scanned {success_count}/{len(tokens)} tokens successfully")
                
            except Exception as e:
                if self.debug:
                    print(f"‚ùå Auto-scan loop error: {e}")
            
            await asyncio.sleep(self.scan_interval)
    
    async def start_auto_scan(self):
        if self.is_scanning:
            return {"success": False, "message": "DexScreener scanner already running"}
        
        self.is_scanning = True
        self.scan_task = asyncio.create_task(self._auto_scan_loop())
        return {"success": True, "message": "DexScreener scanner started"}
    
    async def stop_auto_scan(self):
        if not self.is_scanning:
            return {"success": False, "message": "DexScreener scanner not running"}
        
        self.is_scanning = False
        if self.scan_task:
            self.scan_task.cancel()
            try:
                await self.scan_task
            except asyncio.CancelledError:
                pass
            self.scan_task = None
        
        return {"success": True, "message": "DexScreener scanner stopped"}
    
    def get_status(self):
        return {
            "is_scanning": self.is_scanning,
            "scan_interval": self.scan_interval,
            "batch_size": self.batch_size
        }

dexscreener_instance: Optional[DexScreenerAnalyzer] = None

async def get_dexscreener_analyzer() -> DexScreenerAnalyzer:
    global dexscreener_instance
    if dexscreener_instance is None:
        dexscreener_instance = DexScreenerAnalyzer(debug=True)
        await dexscreener_instance.ensure_connection()
        await dexscreener_instance.ensure_session()
    return dexscreener_instance


async def refresh_missing_token_pairs(debug: bool = True, delay_seconds: float = 1.0, batch_size: int = 5, max_tokens: int = None, force_rescan: bool = False) -> Dict:
    """
    –†—É—á–Ω–µ –∑–∞–ø–æ–≤–Ω–µ–Ω–Ω—è token_pair –¥–ª—è —Ç–æ–∫–µ–Ω—ñ–≤ –±–µ–∑ —Ç–æ—Ä–≥–æ–≤–æ—ó –ø–∞—Ä–∏
    
    –ü—Ä–æ—Ö–æ–¥–∏—Ç—å –ø–æ –≤—Å—ñ—Ö —Ç–æ–∫–µ–Ω–∞—Ö –ë–ï–ó token_pair —ñ –∑–∞–ø–∏—Ç—É—î DexScreener API.
    Rate limit: 5 –∑–∞–ø–∏—Ç—ñ–≤/—Å–µ–∫—É–Ω–¥—É (batch_size=5, delay=1.0s)
    
    Args:
        debug: –í–∏–≤–æ–¥–∏—Ç–∏ –¥–µ—Ç–∞–ª—å–Ω—ñ –ª–æ–≥–∏
        delay_seconds: –ó–∞—Ç—Ä–∏–º–∫–∞ –º—ñ–∂ –±–∞—Ç—á–∞–º–∏ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö (–∑–∞ –∑–∞–º–æ–≤—á—É–≤–∞–Ω–Ω—è–º 1.0s)
        batch_size: –ö—ñ–ª—å–∫—ñ—Å—Ç—å —Ç–æ–∫–µ–Ω—ñ–≤ –Ω–∞ –±–∞—Ç—á (–∑–∞ –∑–∞–º–æ–≤—á—É–≤–∞–Ω–Ω—è–º 5 - rate limit)
        max_tokens: –ú–∞–∫—Å–∏–º—É–º —Ç–æ–∫–µ–Ω—ñ–≤ –¥–ª—è –æ–±—Ä–æ–±–∫–∏ (None = –≤—Å—ñ —Ç–æ–∫–µ–Ω–∏). –î–ª—è —Ç–µ—Å—Ç—É–≤–∞–Ω–Ω—è.
        force_rescan: –Ø–∫—â–æ True, —Å–∫–∞–Ω—É—î –Ω–∞–≤—ñ—Ç—å —Ç–æ–∫–µ–Ω–∏ –¥–µ check_dexscreener >= 3
    
    Returns:
        Dict –∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏: total_tokens, processed_tokens, success_count, failed_count
    
    Usage:
        # –¢—ñ–ª—å–∫–∏ —Ç–æ–∫–µ–Ω–∏ –ë–ï–ó token_pair (skip –≥–æ—Ç–æ–≤–∏—Ö):
        python3 -c "import asyncio; from _v2_analyzer_dexscreener import refresh_missing_token_pairs; asyncio.run(refresh_missing_token_pairs())"
        
        # –ü–µ—Ä—à—ñ 10 —Ç–æ–∫–µ–Ω—ñ–≤ (—Ç–µ—Å—Ç):
        python3 -c "import asyncio; from _v2_analyzer_dexscreener import refresh_missing_token_pairs; asyncio.run(refresh_missing_token_pairs(max_tokens=10))"
        
        # Force rescan (–Ω–∞–≤—ñ—Ç—å —è–∫—â–æ check_dexscreener >= 3):
        python3 -c "import asyncio; from _v2_analyzer_dexscreener import refresh_missing_token_pairs; asyncio.run(refresh_missing_token_pairs(force_rescan=True))"
    """
    analyzer = DexScreenerAnalyzer(debug=debug)
    
    try:
        await analyzer.ensure_connection()
        await analyzer.ensure_session()
        
        # –û—Ç—Ä–∏–º—É—î–º–æ —Ç–æ–∫–µ–Ω–∏ –ë–ï–ó token_pair
        pool = await get_db_pool()
        async with pool.acquire() as conn:
            if force_rescan:
                # –í—Å—ñ —Ç–æ–∫–µ–Ω–∏ –ë–ï–ó token_pair (–Ω–∞–≤—ñ—Ç—å –∑ check_dexscreener >= 3)
                rows = await conn.fetch("""
                    SELECT id, token_address, check_dexscreener
                    FROM token_ids
                    WHERE token_pair IS NULL OR token_pair = ''
                    ORDER BY created_at ASC
                """)
            else:
                # –¢—ñ–ª—å–∫–∏ —Ç–æ–∫–µ–Ω–∏ –¥–µ check_dexscreener < 3 (—â–µ —î —Å–ø—Ä–æ–±–∏)
                rows = await conn.fetch("""
                    SELECT id, token_address, check_dexscreener
                    FROM token_ids
                    WHERE (token_pair IS NULL OR token_pair = '')
                      AND check_dexscreener < 3
                    ORDER BY created_at ASC
                """)
        
        tokens = [
            {
                "token_id": row['id'],
                "token_address": row['token_address'],
                "check_dexscreener": row['check_dexscreener']
            }
            for row in rows
        ]
        
        if not tokens:
            print("‚ö†Ô∏è  –¢–æ–∫–µ–Ω—ñ–≤ –ë–ï–ó token_pair –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ –≤ –ë–î")
            return {
                "success": True,
                "total_tokens": 0,
                "processed_tokens": 0,
                "success_count": 0,
                "failed_count": 0
            }
        
        # –û–±–º–µ–∂—É—î–º–æ –∫—ñ–ª—å–∫—ñ—Å—Ç—å —Ç–æ–∫–µ–Ω—ñ–≤ –¥–ª—è —Ç–µ—Å—Ç—É–≤–∞–Ω–Ω—è
        if max_tokens:
            tokens = tokens[:max_tokens]
        
        print(f"\n{'='*80}")
        print(f"üöÄ –ó–ê–ü–û–í–ù–ï–ù–ù–Ø TOKEN_PAIR –î–õ–Ø –¢–û–ö–ï–ù–Ü–í –ë–ï–ó –¢–û–†–ì–û–í–û–á –ü–ê–†–ò")
        print(f"{'='*80}")
        print(f"üìä –ó–Ω–∞–π–¥–µ–Ω–æ —Ç–æ–∫–µ–Ω—ñ–≤ –ë–ï–ó token_pair: {len(tokens)}")
        if not force_rescan:
            print(f"‚è≠Ô∏è  –ü—Ä–æ–ø—É—Å–∫–∞—î–º–æ —Ç–æ–∫–µ–Ω–∏ –∑ check_dexscreener >= 3")
        else:
            print(f"üîÑ Force rescan: —Å–∫–∞–Ω—É–≤–∞–Ω–Ω—è –í–°–Ü–• —Ç–æ–∫–µ–Ω—ñ–≤ –ë–ï–ó token_pair")
        print(f"‚è±Ô∏è  –ó–∞—Ç—Ä–∏–º–∫–∞ –º—ñ–∂ –±–∞—Ç—á–∞–º–∏: {delay_seconds}s")
        print(f"üì¶ –†–æ–∑–º—ñ—Ä –±–∞—Ç—á—É: {batch_size} —Ç–æ–∫–µ–Ω—ñ–≤/—Å–µ–∫ (rate limit)")
        if max_tokens:
            print(f"üß™ –¢–ï–°–¢–û–í–ò–ô –†–ï–ñ–ò–ú: –æ–±—Ä–æ–±–ª—é—î–º–æ —Ç—ñ–ª—å–∫–∏ {max_tokens} —Ç–æ–∫–µ–Ω–∏")
        print(f"{'='*80}\n")
        
        success_count = 0
        failed_count = 0
        processed_tokens = 0
        
        # –û–±—Ä–æ–±–ª—è—î–º–æ –ø–æ –±–∞—Ç—á–∞—Ö (5 —Ç–æ–∫–µ–Ω—ñ–≤ –∑–∞ 1 —Å–µ–∫—É–Ω–¥—É)
        total_batches = (len(tokens) + batch_size - 1) // batch_size
        
        for batch_idx in range(total_batches):
            start_idx = batch_idx * batch_size
            end_idx = min(start_idx + batch_size, len(tokens))
            batch = tokens[start_idx:end_idx]
            
            print(f"\n{'‚îÄ'*80}")
            print(f"üì¶ –ë–∞—Ç—á {batch_idx + 1}/{total_batches} ({len(batch)} —Ç–æ–∫–µ–Ω—ñ–≤)")
            print(f"{'‚îÄ'*80}")
            
            # –û–±—Ä–æ–±–ª—è—î–º–æ –≤—Å—ñ —Ç–æ–∫–µ–Ω–∏ –≤ –±–∞—Ç—á—ñ –ø–∞—Ä–∞–ª–µ–ª—å–Ω–æ
            tasks = []
            for token in batch:
                tasks.append(analyzer.scan_token(token))
            
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            # –ü—ñ–¥—Ä–∞—Ö–æ–≤—É—î–º–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏
            for idx, result in enumerate(results):
                token = batch[idx]
                processed_tokens += 1
                
                if isinstance(result, Exception):
                    failed_count += 1
                    if debug:
                        print(f"‚ùå Token {token['token_address'][:16]}... failed: {result}")
                elif result is True:
                    success_count += 1
                    if debug:
                        print(f"‚úÖ Token {token['token_address'][:16]}... success")
                else:
                    failed_count += 1
                    if debug:
                        print(f"‚ö†Ô∏è  Token {token['token_address'][:16]}... no pair found")
            
            print(f"üìä –ë–∞—Ç—á —Ä–µ–∑—É–ª—å—Ç–∞—Ç: {sum(1 for r in results if r is True)}/{len(batch)} —É—Å–ø—ñ—à–Ω–æ")
            
            # –ó–∞—Ç—Ä–∏–º–∫–∞ –º—ñ–∂ –±–∞—Ç—á–∞–º–∏ (–∫—Ä—ñ–º –æ—Å—Ç–∞–Ω–Ω—å–æ–≥–æ)
            if batch_idx < total_batches - 1:
                print(f"‚è≥ –ó–∞—Ç—Ä–∏–º–∫–∞ {delay_seconds}s –ø–µ—Ä–µ–¥ –Ω–∞—Å—Ç—É–ø–Ω–∏–º –±–∞—Ç—á–µ–º...")
                await asyncio.sleep(delay_seconds)
        
        print(f"\n{'='*80}")
        print(f"üéâ –ó–ê–ü–û–í–ù–ï–ù–ù–Ø –ó–ê–í–ï–†–®–ï–ù–û")
        print(f"{'='*80}")
        print(f"‚úÖ –û–±—Ä–æ–±–ª–µ–Ω–æ —Ç–æ–∫–µ–Ω—ñ–≤: {processed_tokens}/{len(tokens)}")
        print(f"‚úÖ –£—Å–ø—ñ—à–Ω–æ –∑–∞–ø–æ–≤–Ω–µ–Ω–æ: {success_count}")
        print(f"‚ùå –ü–æ–º–∏–ª–æ–∫/–Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ: {failed_count}")
        print(f"{'='*80}\n")
        
        return {
            "success": True,
            "total_tokens": len(tokens),
            "processed_tokens": processed_tokens,
            "success_count": success_count,
            "failed_count": failed_count
        }
        
    finally:
        await analyzer.close()
